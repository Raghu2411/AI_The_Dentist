{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoiZ-SLKLKj4",
        "outputId": "64170d47-f4d7-42cf-c215-bc38b9d1d997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import uuid\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "old_zip_path = '/content/drive/MyDrive/Dental Dissertation/before_filtered_opg_dataset_for_LLM2.zip'\n",
        "new_zip_path = '/content/drive/MyDrive/Dental Dissertation/rabia_new_data3.zip'\n",
        "\n",
        "base_dir = '/content/dataset_merging'\n",
        "old_dataset_dir = os.path.join(base_dir, 'old_dataset')\n",
        "new_data_dir = os.path.join(base_dir, 'new_data')\n",
        "merged_dataset_dir = '/content/merged_opg_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miooVg5eLKhW",
        "outputId": "2e6a5827-4fb4-46c5-927a-bef4ff13638e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning up old directories...\n",
            "Unzipping OLD dataset to /content/dataset_merging/old_dataset...\n",
            "Unzipping NEW dataset to /content/dataset_merging/new_data...\n"
          ]
        }
      ],
      "source": [
        "# Cleanup and Unzip\n",
        "print(\"Cleaning up old directories...\")\n",
        "shutil.rmtree(base_dir, ignore_errors=True)\n",
        "shutil.rmtree(merged_dataset_dir, ignore_errors=True)\n",
        "\n",
        "print(f\"Unzipping OLD dataset to {old_dataset_dir}...\")\n",
        "with zipfile.ZipFile(old_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(old_dataset_dir)\n",
        "\n",
        "print(f\"Unzipping NEW dataset to {new_data_dir}...\")\n",
        "with zipfile.ZipFile(new_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(new_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM4s9CjPLKew",
        "outputId": "01f5a11b-25f1-492a-bd06-201c64f955b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Locating old dataset contents in /content/dataset_merging/old_dataset...\n",
            "Found single top-level folder: merged_opg_dataset. Copying from there.\n",
            "Copying old dataset from /content/dataset_merging/old_dataset/merged_opg_dataset to /content/merged_opg_dataset...\n",
            "Base copy complete. 'valid' and 'test' folders are now in place.\n"
          ]
        }
      ],
      "source": [
        "# Prepare Merged Folder\n",
        "# find the train/valid/test folders.\n",
        "\n",
        "print(f\"Locating old dataset contents in {old_dataset_dir}...\")\n",
        "unzipped_files = os.listdir(old_dataset_dir)\n",
        "base_copy_dir = None\n",
        "\n",
        "if 'train' in unzipped_files and 'valid' in unzipped_files:\n",
        "    print(\"Found train/valid folders at the root. Copying from base.\")\n",
        "    base_copy_dir = old_dataset_dir\n",
        "elif len(unzipped_files) == 1 and os.path.isdir(os.path.join(old_dataset_dir, unzipped_files[0])):\n",
        "    potential_base = os.path.join(old_dataset_dir, unzipped_files[0])\n",
        "    if 'train' in os.listdir(potential_base) and 'valid' in os.listdir(potential_base):\n",
        "        print(f\"Found single top-level folder: {unzipped_files[0]}. Copying from there.\")\n",
        "        base_copy_dir = potential_base\n",
        "    else:\n",
        "        print(f\"Error: Single folder '{unzipped_files[0]}' found, but it doesn't contain train/valid.\")\n",
        "        raise Exception(\"Old dataset structure not recognized.\")\n",
        "else:\n",
        "    print(f\"Error: Could not determine the structure of the old zip. Found: {unzipped_files}\")\n",
        "    raise Exception(\"Old dataset structure not recognized. Stopping.\")\n",
        "\n",
        "print(f\"Copying old dataset from {base_copy_dir} to {merged_dataset_dir}...\")\n",
        "# create merged_opg_dataset/train, /valid, /test\n",
        "shutil.copytree(base_copy_dir, merged_dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4BJXvjbLKcH",
        "outputId": "7fc13c17-0bb3-4922-dbf3-fbc290ca25ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master annotation file: /content/merged_opg_dataset/train/_annotations.coco.json\n",
            "New annotation file: /content/dataset_merging/new_data/train/_annotations.coco.json\n"
          ]
        }
      ],
      "source": [
        "# Define Key Paths for Merging\n",
        "master_train_folder = os.path.join(merged_dataset_dir, 'train')\n",
        "master_coco_path = os.path.join(master_train_folder, '_annotations.coco.json')\n",
        "\n",
        "# Find the train filder in the new dataset.\n",
        "new_train_folder = os.path.join(new_data_dir, 'train')\n",
        "# Handle if new data *also* has a top-level folder\n",
        "if not os.path.exists(new_train_folder):\n",
        "    new_data_root = os.path.join(new_data_dir, os.listdir(new_data_dir)[0])\n",
        "    new_train_folder = os.path.join(new_data_root, 'train')\n",
        "\n",
        "new_coco_path = os.path.join(new_train_folder, '_annotations.coco.json')\n",
        "\n",
        "print(f\"Master annotation file: {master_coco_path}\")\n",
        "print(f\"New annotation file: {new_coco_path}\")\n",
        "\n",
        "if not os.path.exists(master_coco_path):\n",
        "    raise Exception(f\"Master COCO file not found at: {master_coco_path}\")\n",
        "if not os.path.exists(new_coco_path):\n",
        "    raise Exception(f\"New COCO file not found at: {new_coco_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVLKa2-3LKZO",
        "outputId": "76c6c967-7af9-4b70-916b-45c701d1bf19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading COCO annotation files...\n",
            "Master dataset: 1768 images, 60688 annotations, 63 categories.\n",
            "New dataset: 105 images, 993 annotations, 46 categories.\n"
          ]
        }
      ],
      "source": [
        "# Load COCO Files\n",
        "print(\"Loading COCO annotation files...\")\n",
        "with open(master_coco_path, 'r') as f:\n",
        "    master_coco = json.load(f)\n",
        "\n",
        "with open(new_coco_path, 'r') as f:\n",
        "    new_coco = json.load(f)\n",
        "\n",
        "print(f\"Master dataset: {len(master_coco['images'])} images, {len(master_coco['annotations'])} annotations, {len(master_coco['categories'])} categories.\")\n",
        "print(f\"New dataset: {len(new_coco['images'])} images, {len(new_coco['annotations'])} annotations, {len(new_coco['categories'])} categories.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-nT06avLwDV",
        "outputId": "d934d241-059d-466f-9203-da347ee8e896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging categories...\n",
            "Total categories after merge: 63\n"
          ]
        }
      ],
      "source": [
        "# Merge Categories\n",
        "print(\"Merging categories...\")\n",
        "\n",
        "# A lookup for master categories by name\n",
        "master_cat_map = {cat['name']: cat for cat in master_coco['categories']}\n",
        "max_master_cat_id = 0\n",
        "if master_coco['categories']:\n",
        "    max_master_cat_id = max(cat['id'] for cat in master_coco['categories'])\n",
        "\n",
        "# This map translate NEW_cat_id -> MASTER_cat_id\n",
        "category_translation_map = {}\n",
        "\n",
        "for new_cat in new_coco['categories']:\n",
        "    new_name = new_cat['name']\n",
        "    new_id = new_cat['id']\n",
        "\n",
        "    if new_name in master_cat_map:\n",
        "        # Category already exists, map new ID to master ID\n",
        "        category_translation_map[new_id] = master_cat_map[new_name]['id']\n",
        "    else:\n",
        "        print(f\"Adding new category: '{new_name}'\")\n",
        "        max_master_cat_id += 1\n",
        "        new_master_entry = {'id': max_master_cat_id, 'name': new_name}\n",
        "\n",
        "        master_coco['categories'].append(new_master_entry)\n",
        "        master_cat_map[new_name] = new_master_entry # Add to lookup\n",
        "        category_translation_map[new_id] = max_master_cat_id\n",
        "\n",
        "print(f\"Total categories after merge: {len(master_coco['categories'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHCsp-hHLybM",
        "outputId": "209dda87-6735-4f2f-ad0b-2ed422aeacc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging images and annotations...\n"
          ]
        }
      ],
      "source": [
        "# Merge Images & Annotations\n",
        "print(\"Merging images and annotations...\")\n",
        "\n",
        "max_img_id = 0\n",
        "if master_coco['images']:\n",
        "    max_img_id = max([img['id'] for img in master_coco['images']] or [0])\n",
        "\n",
        "max_ann_id = 0\n",
        "if master_coco['annotations']:\n",
        "    max_ann_id = max([ann['id'] for ann in master_coco['annotations']] or [0])\n",
        "\n",
        "# translate NEW_image_id -> new MASTER_image_id\n",
        "image_id_translation_map = {}\n",
        "\n",
        "for new_img in new_coco['images']:\n",
        "    old_img_id = new_img['id']\n",
        "    file_name = new_img['file_name']\n",
        "\n",
        "    src_img_path = os.path.join(new_train_folder, file_name)\n",
        "    dst_img_path = os.path.join(master_train_folder, file_name)\n",
        "\n",
        "    if not os.path.exists(src_img_path):\n",
        "        print(f\"Warning: Image file not found, skipping: {src_img_path}\")\n",
        "        continue\n",
        "\n",
        "    if os.path.exists(dst_img_path):\n",
        "        unique_id = uuid.uuid4().hex[:8]\n",
        "        base, ext = os.path.splitext(file_name)\n",
        "        new_file_name = f\"{base}_new_{unique_id}{ext}\"\n",
        "        print(f\"File clash for '{file_name}'. Renaming new file to '{new_file_name}'.\")\n",
        "        dst_img_path = os.path.join(master_train_folder, new_file_name)\n",
        "        new_img['file_name'] = new_file_name \n",
        "\n",
        "    shutil.copy(src_img_path, dst_img_path)\n",
        "\n",
        "    # Re-ID the image and add to master list\n",
        "    max_img_id += 1\n",
        "    new_master_img_id = max_img_id\n",
        "\n",
        "    image_id_translation_map[old_img_id] = new_master_img_id\n",
        "    new_img['id'] = new_master_img_id\n",
        "    master_coco['images'].append(new_img)\n",
        "\n",
        "# Process Annotations\n",
        "for new_ann in new_coco['annotations']:\n",
        "    old_img_id = new_ann['image_id']\n",
        "    old_cat_id = new_ann['category_id']\n",
        "\n",
        "    if old_img_id not in image_id_translation_map:\n",
        "        continue # Image was skipped (e.g., file not found)\n",
        "    if old_cat_id not in category_translation_map:\n",
        "        print(f\"Warning: Skipping annotation {new_ann['id']} with unknown category {old_cat_id}\")\n",
        "        continue # Category was not in the new list or master list\n",
        "\n",
        "    # Re-ID the annotation\n",
        "    max_ann_id += 1\n",
        "    new_ann['id'] = max_ann_id\n",
        "\n",
        "    # Translate the image and category IDs\n",
        "    new_ann['image_id'] = image_id_translation_map[old_img_id]\n",
        "    new_ann['category_id'] = category_translation_map[old_cat_id]\n",
        "\n",
        "    master_coco['annotations'].append(new_ann)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qX9FctL0kw",
        "outputId": "b4741fc3-e551-4918-83a2-ddd7785ff1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving merged annotation file...\n",
            "\n",
            "--- MERGE COMPLETE ---\n",
            "Final merged dataset location: /content/merged_opg_dataset\n",
            "Total images in 'train': 1873\n",
            "Total annotations in 'train': 61681\n",
            "Total categories: 63\n"
          ]
        }
      ],
      "source": [
        "# Save the Merged COCO file\n",
        "print(\"Saving merged annotation file...\")\n",
        "with open(master_coco_path, 'w') as f:\n",
        "    json.dump(master_coco, f, indent=2)\n",
        "\n",
        "print(\"\\n--- MERGE COMPLETE ---\")\n",
        "print(f\"Final merged dataset location: {merged_dataset_dir}\")\n",
        "print(f\"Total images in 'train': {len(master_coco['images'])}\")\n",
        "print(f\"Total annotations in 'train': {len(master_coco['annotations'])}\")\n",
        "print(f\"Total categories: {len(master_coco['categories'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TI0qECxL26T",
        "outputId": "2a3b3060-c91d-4ab9-f0f0-57eaa76bbc4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to zip the folder: ./merged_opg_dataset\n",
            "Zipping complete in 40.58 seconds.\n",
            "Copying before_filtered_opg_dataset_for_LLM3.zip to your Google Drive...\n",
            "cp: target 'Dissertation/before_filtered_opg_dataset_for_LLM3.zip' is not a directory\n",
            "Copying complete in 0.11 seconds.\n",
            "\n",
            "âœ… Success!\n",
            "Your new dataset is saved as a zip file at: /content/drive/MyDrive/Dental Dissertation/before_filtered_opg_dataset_for_LLM3.zip\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "folder_to_zip = './merged_opg_dataset'\n",
        "zip_file_name = 'before_filtered_opg_dataset_for_LLM3.zip'\n",
        "drive_destination_path = f'/content/drive/MyDrive/Dental Dissertation/{zip_file_name}'\n",
        "\n",
        "\n",
        "print(f\"Zipping the folder: {folder_to_zip}\")\n",
        "start_time = time.time()\n",
        "\n",
        "!zip -r -q {zip_file_name} {folder_to_zip}\n",
        "\n",
        "zip_time = time.time()\n",
        "print(f\"Zipping complete in {zip_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Copy the single zip file to Google Drive\n",
        "print(f\"Copying {zip_file_name} to your Google Drive...\")\n",
        "!cp {zip_file_name} {drive_destination_path}\n",
        "\n",
        "copy_time = time.time()\n",
        "print(f\"Copying complete in {copy_time - zip_time:.2f} seconds.\")\n",
        "\n",
        "print(\"\\n Success!\")\n",
        "print(f\"New dataset is saved as a zip file at: {drive_destination_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xgzMrbseMW47",
        "outputId": "def10dcc-ee5e-428f-9c76-a9d64f69e175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing to download before_filtered_opg_dataset_for_LLM3.zip...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6f030e0b-565a-48c2-8660-acdeb731b901\", \"before_filtered_opg_dataset_for_LLM3.zip\", 727175787)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "zip_file_name = 'before_filtered_opg_dataset_for_LLM3.zip'\n",
        "\n",
        "print(f\"Preparing to download {zip_file_name}...\")\n",
        "files.download(zip_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObT-OSTJMxhb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
